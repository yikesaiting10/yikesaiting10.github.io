---
title:    IO多路复用
subtitle:   select、poll和epoll
header-img:   img/network_post.jpg
author:   zql
date:   2020-04-25
catalog:    true
tags:
    - 计算机网络
---
### 目录
 - 四种I/O
 - select
 - poll
 - epoll  

**文件描述符：**socket建立的连接，fd。  
### 四种I/O  
**阻塞I/O**  
阻塞I/O函数只有当有事件发生才会继续往下运行，如果没有事件发生会一直阻塞在这儿，比如accept()  
**非阻塞I/O**  
非阻塞I/O函数会一直查询是否有事件发生，如果没有事件发生，程序也可以去执行其他命令，如果有事件发生，就卡在这儿把数据从内核缓冲区取到用户缓冲区。  
**异步I/O**  
异步I/O函数会指定一个缓冲区和一个回调函数，该函数会立即返回。其余判断交给操作系统，操作系统会判断数据是否到来，如果数据到来了，操作系统会把数据
拷贝到所提供的缓冲区里，然后调用所指定的这个回调函数来通知程序。  
**同步I/O**  
select，poll，epoll  
1.调用select()判断有没有数据，有数据，走下来，没数据卡在那里；
2.select()返回之后，用recvfrom()去取数据；当然取数据的时候也会卡那么一下；
同步I/O感觉更麻烦，要调用两个函数才能把数据拿到手，但是同步I/O和阻塞式I/O比，就是所谓的I/O复用【用两个函数来收数据的优势】能力；
3.I/O复用  
所谓I/O复用，就是多个socket【多个TCP连接】可以弄成一捆【一堆】，可以用select这种同步I/O函数在这等数据；
select()的能力是等多条TCP连接上的任意一条有数据来；，然后哪条TCP有数据来，再用具体的比如recvfrom()去收。
所以，这种调用一个函数能够判断一堆TCP连接是否来数据的这种能力，叫I/O复用，英文I/O multiplexing【I/O多路复用】  
### select  
（1）使用copy_from_user从用户空间拷贝fd_set到内核空间  
（2）注册回调函数__pollwait  
（3）遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）  
（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数  
（5）__pollwait的主要工作就是把current（当前进程）挂到fd对应的设备等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，
其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。
在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。  
（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值  
（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout使调用select的进程（也就是current）进入睡眠。
当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，
则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。  
（8）把fd_set从内核空间拷贝到用户空间。  
  ![avatar](/img/io_post.jpg)
 **select的几大缺点：**  
（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大  
（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大  
（3）select支持的文件描述符数量太小了，默认是1024  
### poll  
poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，
其他的都差不多,管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。
poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，
而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。  
### epoll
epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？
在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。
而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；
epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。  
对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），
会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。  
对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，
而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，
就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。
epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。  
对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat/proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。  
**epoll_create()**  
格式：int epoll_create(int size)  
功能：创建一个epoll对象，返回该对象的描述符【文件描述符】，这个描述符就代表这个epoll对象，后续会用到，这个epoll对象最终要用close(),因为文件描述符/句柄总是关闭的。  
原理：
1.struct eventpoll *ep = (struct eventpoll*)calloc(1, sizeof(struct eventpoll))   
2.rbr结构成员：代表一颗红黑树的根节点[刚开始指向空],把rbr理解成红黑树的根节点的指针  
3.红黑树，用来保存 键【数字】/值【结构】，能够快速的通过你给key，把整个的键/值取出来  
4.rdlist结构成员：代表一个双向链表的表头指针  
总结：创建了一个eventpoll结构对象，被系统保存起来  
rbr成员被初始化成指向一颗红黑树的根【有了一个红黑树】  
rdlist成员被初始化成指向一个双向链表的根【有了双向链表】  
**epoll_ctl()**  
格式：int epoll_ctl(int efpd,int op,int sockid,struct epoll_event * event)  
功能：把一个socket以及这个socket相关的事件添加到这个epoll对象描述符中去，目的就是通过这个epoll对象来监视这个socket【客户端的TCP连接】上数据的来往情况。我们把感兴趣的事件通过epoll_ctl（）添加到系统，当这些事件来的时候，系统会通知我们。  
efpd：epoll_create()返回的epoll对象描述符  
op：动作，添加/删除/修改 ，对应数字是1,2,3， EPOLL_CTL_ADD, EPOLL_CTL_DEL ,EPOLL_CTL_MOD  
    EPOLL_CTL_ADD添加事件：往红黑树上添加一个节点，每个客户端连入服务器后，服务器都会产生 一个对应的socket，每个连接这个socket值都不重复。所以，这个socket就是红黑树中的key，把这个节点添加到红黑树上去。  
	EPOLL_CTL_MOD：修改事件；用了EPOLL_CTL_ADD把节点添加到红黑树上之后，才存在修改。  
	EPOLL_CTL_DEL：是从红黑树上把这个节点干掉；这会导致这个socket【这个tcp链接】上无法收到任何系统通知事件。  
	sockid：表示客户端连接，就是从accept()获取的id，这个是红黑树里边的key。  
	event：事件信息，这里包括的是一些事件信息；EPOLL_CTL_ADD和EPOLL_CTL_MOD都要用到这个event参数里边的事件信息。  
原理：  
1.epi = (struct epitem*)calloc(1, sizeof(struct epitem))  
2.epi = RB_INSERT(_epoll_rb_socket, &ep->rbr, epi);【EPOLL_CTL_ADD】增加节点到红黑树中
	  epitem.rbn ，代表三个指针，分别指向红黑树的左子树，右子树，父亲  
3.epi = RB_REMOVE(_epoll_rb_socket, &ep->rbr, epi);【EPOLL_CTL_DEL】，从红黑树中把节点干掉  
	  EPOLL_CTL_MOD，找到红黑树节点，修改这个节点中的内容  
总结：  
EPOLL_CTL_ADD：等价于往红黑树中增加节点  
EPOLL_CTL_DEL：等价于从红黑树中删除节点  
EPOLL_CTL_MOD：等价于修改已有的红黑树的节点  
**epoll_wait()**  
格式：int epoll_wait(int epfd,struct epoll_event *events,int maxevents,int timeout)  
功能：阻塞一小段时间并等待事件发生，返回事件集合，也就是获取内核的事件通知。遍历这个双向链表，把这个双向链表里边的节点数据拷贝出去，拷贝完毕的就从双向链表里移除。因为双向链表里记录的是所有有数据/有事件的socket【TCP连接】。  
epfd：是epoll_create()返回的epoll对象描述符。  
参数events：是内存，也是数组，长度是maxevents，表示此次epoll_wait调用可以收集到的maxevents个已经继续【已经准备好的】的读写事件。就是返回的是实际发生事件的tcp连接数目。  
timeout：阻塞等待的时长。  
epitem结构设计的高明之处：既能够作为红黑树中的节点，又能够作为双向链表中的节点。  
内核向双向链表增加节点。一般有四种情况，会使操作系统把节点插入到双向链表中:  
1.客户端完成三路握手；服务器要accept()  
2.当客户端关闭连接，服务器也要调用close()关闭  
3.客户端发送数据来的；服务器要调用read(),recv()函数来收数据  
4.当可以发送数据时；服务武器可以调用send(),write()  

epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式  
1. LT模式  
LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。  
2. ET模式  
ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)  
ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。  
3、LT模式与ET模式的区别如下：  
LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。  
ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。  
